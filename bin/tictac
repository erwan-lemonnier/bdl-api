#!/usr/bin/env python3
import os
import sys
import json
import time
import logging
import requests
import schedule
import click
import flask
import traceback
from pymacaron.config import get_config
from pymacaron.auth import generate_token
from pymacaron.utils import timenow, to_epoch
from pymacaron.crash import set_error_reporter
from pymacaron.crash import report_error
from pymacaron.crash import populate_error_report


log = logging.getLogger(__name__)


app = flask.Flask(__name__)


# Setup logging
log.setLevel(logging.DEBUG)
handler = logging.StreamHandler(sys.stdout)
handler.setLevel(logging.DEBUG)
# formatter = logging.Formatter('%(message)s')
# handler.setFormatter(formatter)
log.addHandler(handler)
logging.getLogger('urllib3.connectionpool').setLevel(logging.INFO)


PATH_LIBS = os.path.join(os.path.dirname(os.path.realpath(__file__)), '..')
sys.path.append(PATH_LIBS)

config_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'pym-config.yaml')
get_config(config_path)

from bdl.exceptions import bdl_error_reporter
from bdl.io.slack import do_slack


# If set, will override the default api|scraper.bazardelux.com target urls
HOST = None


def call_api(method, url, data=None, ignore_error=False):
    headers = {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer %s' % generate_token('scheduler', data={}, expire_in=86400)
    }
    if method == 'POST':
        r = requests.post(url, data=json.dumps(data), headers=headers)
    else:
        r = requests.get(url, headers=headers)
    j = r.json()
    log.info("=> Got: %s" % json.dumps(j, indent=4))
    # TODO: if j is an error, raise it as an exception


#
# SCAN
#

LAST_SCAN_EPOCHS = {
    # source: epoch last scan
}

def scan_source(source):
    source = source.upper()
    log.info("=> Lauching scan of source %s" % source)

    domain = HOST if HOST else 'https://api.bazardelux.com'

    global LAST_SCAN_EPOCHS

    # By default, scan 1 day back in time
    epoch_oldest = to_epoch(timenow) - 86600

    if source in LAST_SCAN_EPOCHS:
        # Scan announces until back to the publication times later than when the last scan started
        epoch_oldest = LAST_SCAN_EPOCHS[source]
    else:
        # Find out epoch of last scan
        j = call_api('GET', '%s/v1/search/latest?source?%s' % (domain, source), ignore_error=True)
        if 'bdlitem' in j and 'epoch_published' in j['bdlitem']:
            epoch_oldest = j['bdlitem']['epoch_published']
            # If j is an error, just use the default epoch_oldest

    # And launch a scan
    log.info("Scanning back to epoch %s (%s)" % (epoch_oldest, time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(epoch_oldest))))
    call_api(
        'POST',
        'https://crawler.bazardelux.com/v1/crawler/scan/%s' % source.lower(),
        {
            'source': source,
            'epoch_oldest': epoch_oldest,
        },
    )

    # Remember the new epoch_oldest
    LAST_SCAN_EPOCHS[source] = epoch_oldest

    do_slack(
        "Scheduler: launched scan of source %s" % source,
        channel=get_config().slack_scheduler_channel,
    )


def update_sitemap():
    log.info("=> Lauching sitemap update")

    domain = HOST if HOST else 'https://api.bazardelux.com'

    call_api('GET', '%s/v1/bdl/sitemap/update' % domain)

    do_slack(
        "Scheduler: Updated sitemap",
        channel=get_config().slack_scheduler_channel,
    )



def clean_source(source):
    source = source.upper()
    log.info("=> Cleaning up oldest announces from %s" % source)
    # Count how many items are indexed from this source, and retrieve as many
    # of them so that all will be scanned within a week
    # Call v1/parse with the item's urls

    do_slack(
        "Scheduler: launched cleanup of source %s" % source,
        channel=get_config().slack_scheduler_channel,
    )


@click.command()
@click.option('--scan', required=False, metavar='SOURCE', help="Scan the given source", default=False)
@click.option('--clean', required=False, metavar='SOURCE', help="Remove sold announces from this source", default=False)
@click.option('--sitemap/--no-sitemap', required=False, metavar='', help="Update the bazardelux sitemap", default=False)
@click.option('--host', required=False, metavar='PROTO://HOST:PORT', help="Call this proto://host:port instead of the default bazardelux.com server", default=False)
def main(scan, clean, sitemap, host):
    """Scheduler for bazardelux.com, in charge of running crawlers at regular
    intervals, polling items to remove them when they are sold and updating the
    sitemaps every day.

    Can also be used to launch a single scan, clean or sitemap operation.

    Examples:

    tictac                      # Start the scheduler and keep it running #
    tictac --scan TRADERA       # Scan TRADERA                            #
    tictac --clean TRADERA      # Purge the oldest sold ads from TRADERA  #
    tictac --sitemap            # Update bazardelux's sitemap             #
    tictac --sitemap --host http://127.0.0.1:8080

    """

    # Configure the error reporter and force pymacaron to report errors
    set_error_reporter(bdl_error_reporter)
    os.environ['DO_REPORT_ERROR'] = "1"

    global HOST
    if host:
        HOST = host

    #
    # Check arguments and run single tasks
    #

    if scan or clean:
        if scan and clean:
            log.warn("You must provide only one of --scan or --clean")
            sys.exit(1)
        if scan:
            scan_source(scan)
            sys.exit(0)
        if clean:
            clean_source(clean)
            sys.exit(0)
    elif sitemap:
        update_sitemap()
        sys.exit(0)

    #
    # Default to running the scheduler
    #

    def wrapper(method, *args, **kwargs):
        def scheduler_call():
            try:
                method(*args, **kwargs)
            except Exception as e:
                data = {}
                exc_type, exc_value, exc_traceback = sys.exc_info()
                trace = traceback.format_exception(exc_type, exc_value, exc_traceback, 30)
                data['trace'] = trace
                populate_error_report(data)
                report_error(
                    title='SCHEDULER CRASH: %s%s' % (method.__name__, str(args)),
                    data=data,
                    caught=e,
                    is_fatal=True,
                )

        return scheduler_call

    # schedule.every(3).seconds.do(wrapper(scan_source, 'TRADERA'))
    schedule.every(60).minutes.do(wrapper(scan_source, 'TRADERA'))
    schedule.every().day.at('00:00').do(wrapper(clean_source, 'TRADERA'))
    schedule.every().day.at('12:00').do(wrapper(clean_source, 'TRADERA'))
    schedule.every().day.at('04:00').do(wrapper(update_sitemap))

    log.info("Starting scheduler...")
    while True:
        schedule.run_pending()
        time.sleep(10)

if __name__ == "__main__":
    with app.test_request_context(''):
        main()
